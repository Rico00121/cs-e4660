The computing continuum and multi-continuum computing are vast concepts. I don't think I can offer a definitive perspective in this first study log; I can only offer some reflections on my current understanding.

To understand the computing continuum, I first attempted to read the paper "The Computing Continuum: Past, Present, and Future." I believe understanding its history and past is crucial, as it also gave me a comprehensive understanding of distributed computing. I've already experienced the benefits of this course, which has given me a clearer perspective on the development of distributed computing technology.

The advent of computer networks is the starting point for everything. Through networks, we connect computers, and layering on top of these technologies makes it possible to access vastly larger computing resources. The original idea was to connect a group of computers in a single location via cables to achieve cluster computing. It was here that I first learned about Amdahl's law, a fascinating theorem that made it clear to me that even with the vast amount of computing resources available to support parallel computing, the speedup of your task is always capped. This limit is determined by the proportion of the task that is serialized. This theorem also led me to realize two common problems with all distributed computing technologies: network and task scheduling overhead. Grid computing, on the other hand, focuses on cross-regional and heterogeneous computing. It attempts to connect computers in different locations and of different types by defining common interfaces and intermediate schedulers. In essence, distributed computing as a whole already reveals a desire to isolate users from the underlying computers. However, it still fails to address the task scheduling problem inherent in cluster computing. Its batch processing model makes this type of computing resource unsuitable for handling large numbers of highly concurrent, small tasks. This is why cloud computing and ubiquitous computing were born. Our ultimate goal is to provide sufficient abstraction to allow users and computing resource users to seamlessly and seamlessly access any computing resource they desire, easily leverage it to execute computing tasks of any scale, and receive rapid responses.

Cloud computing appears to have solved all these problems: on-demand computing resources, it abstracts away the complexity of underlying devices, and a large number of AZs globally provide relatively low-latency accessâ€¦ But in my opinion, there are at least two serious issues.

1. Huge Network Communication Costs

Once you choose to use a cloud provider, you limit the location of your computing resources. Longer distances mean greater communication latency. Furthermore, most cloud providers charge for communication traffic, which means frequent requests to your services increase your financial costs. Even within the service itself, the same problem exists. In the serverless era, each invocation of a Serverless Lambda function must go through the internal Lambda control plane to search for other services. Furthermore, cold starts also increase latency to a certain extent.

1. High Cost of Cloud Services

As I just mentioned, even with the lower per-use cost of Serverless, frequent usage can still incur significant financial and time costs. This is why we propose edge computing, which moves computing power to edge devices to minimize unnecessary communication costs.

Looking at the development of distributed computing technology from a technology-agnostic historical perspective, we can see that the scheduling and allocation of computing resources has evolved from focusing solely on a single host to integrating them into large clusters, exploring how to leverage cross-regional clusters, and aiming to build ubiquitous computing resources. Further simplification, attempts to shift focus away from the resources themselves, revealed that performance and communication costs remained issues, leading to a shift in focus towards edge devices, giving rise to edge computing, fog computing, and mobile computing. As these diverse areas flourished, some researchers began to explore how to fully exploit the gaps that could be filled by computing, considering both time and space. This gave rise to in-transit computing. Meanwhile, others transcended the details, connecting them to re-use computing resources across the entire system, attempting to address a new, more holistic problem: how to manage resource allocation and management across the entire link from edge to cloud. This is the computing continuum.

As the professor explained in class, there are two perspectives for thinking about the computing continuum or multi-computing continuum. One is a down-top approach, starting with available infrastructures themselves and combining them to build a common specification for all possible application scenarios, such as a scheduling framework or a set of common APIs plus a scheduler. The other is a top-down approach, examining existing problems and attempting to apply the concepts of the computing continuum to optimize them, achieving higher efficiency and lower costs. Personally, I prefer the second approach. As an individual, I lack the experience and knowledge to design a universal scheduling framework that integrates all existing infrastructures and implements dynamic resource allocation. Therefore, I prefer to study an existing problem and, incorporating my understanding of the computing continuum, consider various approaches to optimize it, such as redesigning the deployment locations of various services and designing specialized resource schedulers for specific business scenarios.

Finally, regarding "*A scenario of service-based applications/systems in multi-continuum computing*", I'm thinking of the context of smart cold chain logistics. In my spare time, I designed and released a remote monitoring system that monitors the real-time status of cold storage and allows remote parameter configuration via an app. There are several pain points in this area. One is the high energy consumption of the equipment, resulting in high operating costs for the refrigeration units. Another is that cold storage is often deployed in complex environments (possibly in mountains, rainforests, or large warehouses), resulting in unstable signals and frequent disconnections. Furthermore, since it's essentially an IoT system, it faces compatibility issues with various hardware devices. Therefore, I believe a multi-continuum perspective can help ensure the safety, energy efficiency, and sustainability of the cold chain. Real-time monitoring of cold storage is a long-term, uninterruptible task. The entire monitoring system involves edge devices, gateways, and the cloud. We can offload some data processing logic to the edge devices and use AI to optimize compressor start/stop and pre-cooling strategies to save electricity costs. Regarding this aspect, I believe that reasonable designs can be made to deploy several small models at the edge, while simultaneously deploying a unified large model in the cloud or fog layer for distributed real-time training. At the same time, we can consider temporarily storing data at the edge in case of unstable signals to ensure system robustness. Furthermore, I can design some universal interfaces to make the entire monitoring system as device-agnostic as possible, achieving so-called interoperability. Regarding the use cases, upon careful consideration, there are actually many areas that can be optimized and expanded, which I will not list here one by one.